{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the video file\n",
    "video_path = \"test2.mp4\"  # Replace with your video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()  # Read a frame\n",
    "\n",
    "    if not ret:\n",
    "        break  # Break the loop if no frame is read (end of video)\n",
    "\n",
    "    cv2.imshow(\"Video Player\", frame)  # Display the frame\n",
    "\n",
    "    # Press 'q' to exit the video\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the video file\n",
    "video_path = \"test2.mp4\"  # Replace with your video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()  # Read a frame\n",
    "\n",
    "    if not ret:\n",
    "        break  # Break the loop if no frame is read (end of video)\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width, _ = frame.shape\n",
    "    center_x, center_y = width // 2, height // 2  # Compute center\n",
    "\n",
    "    # Draw a red dot at the center\n",
    "    cv2.circle(frame, (center_x, center_y), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "    cv2.imshow(\"Video Player\", frame)  # Display the frame\n",
    "\n",
    "    # Press 'q' to exit the video\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the video file\n",
    "#video_path = \"test.mp4\"  # Replace with your video file path\n",
    "\n",
    "# Replace with your phone's IP address and port\n",
    "url = \"https://192.168.185.1:8080/video\"\n",
    "\n",
    "#cap = cv2.VideoCapture(video_path)\n",
    "cap = cv2.VideoCapture(url)\n",
    "\n",
    "# Check if the video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Read the first frame and detect keypoints\n",
    "ret, prev_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error: Could not read the first frame.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "prev_keypoints, prev_descriptors = orb.detectAndCompute(prev_gray, None)\n",
    "\n",
    "# BFMatcher for feature matching\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # End of video\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect keypoints and descriptors in the new frame\n",
    "    keypoints, descriptors = orb.detectAndCompute(gray, None)\n",
    "\n",
    "    if descriptors is not None and prev_descriptors is not None:\n",
    "        # Match descriptors\n",
    "        matches = bf.match(prev_descriptors, descriptors)\n",
    "\n",
    "        # Sort matches by distance (lower is better)\n",
    "        matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "        # Calculate average displacement of keypoints\n",
    "        motion_x = []\n",
    "        motion_y = []\n",
    "        \n",
    "        for m in matches:\n",
    "            pt1 = prev_keypoints[m.queryIdx].pt  # Previous keypoint\n",
    "            pt2 = keypoints[m.trainIdx].pt  # Current keypoint\n",
    "\n",
    "            motion_x.append(pt2[0] - pt1[0])  # X movement\n",
    "            motion_y.append(pt2[1] - pt1[1])  # Y movement\n",
    "\n",
    "        if motion_x and motion_y:\n",
    "            avg_x = np.mean(motion_x)  # Average movement in X\n",
    "            avg_y = np.mean(motion_y)  # Average movement in Y\n",
    "            total_movement = np.sqrt(avg_x**2 + avg_y**2)\n",
    "\n",
    "            # Set threshold for significant movement (adjust as needed)\n",
    "            if total_movement > 120:  \n",
    "                movement_text = \"Camera Moving\"\n",
    "                color = (0, 0, 255)  # Red\n",
    "            else:\n",
    "                movement_text = \"Camera Static\"\n",
    "                color = (0, 255, 0)  # Green\n",
    "            \n",
    "            # Display movement status\n",
    "            cv2.putText(frame, movement_text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Camera Movement Detection\", frame)\n",
    "\n",
    "    # Update previous frame data\n",
    "    prev_gray = gray.copy()\n",
    "    prev_keypoints, prev_descriptors = keypoints, descriptors\n",
    "\n",
    "    # Press 'q' to exit the video\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Replace with your phone's IP address and port\n",
    "url = \"http://192.168.185.1:8080/video\"  # Ensure HTTP, not HTTPS\n",
    "\n",
    "# Function to handle reconnection\n",
    "def reconnect_camera():\n",
    "    while True:\n",
    "        cap = cv2.VideoCapture(url)\n",
    "        if cap.isOpened():\n",
    "            print(\"Reconnected to the camera.\")\n",
    "            return cap\n",
    "        print(\"Reconnecting to the camera...\")\n",
    "        cv2.waitKey(1000)  # Wait 1 sec before retrying\n",
    "\n",
    "# Initialize video capture\n",
    "cap = reconnect_camera()\n",
    "\n",
    "# Initialize ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Read the first frame and detect keypoints\n",
    "ret, prev_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error: Could not read the first frame.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "prev_keypoints, prev_descriptors = orb.detectAndCompute(prev_gray, None)\n",
    "\n",
    "# BFMatcher for feature matching\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Canvas for plotting dots\n",
    "canvas_height, canvas_width = 300, 800  # Canvas size\n",
    "canvas = np.ones((canvas_height, canvas_width, 3), dtype=np.uint8) * 255  # White background\n",
    "dot_position = 50  # Starting X position for dots\n",
    "dot_spacing = 5  # Closer spacing to make a line\n",
    "\n",
    "# Timer to skip first 10 seconds\n",
    "start_time = time.time()\n",
    "skip_time = 10  # Seconds to skip movement detection\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Camera disconnected. Attempting to reconnect...\")\n",
    "        cap = reconnect_camera()\n",
    "        continue  # Skip frame processing until reconnection\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect keypoints and descriptors in the new frame\n",
    "    keypoints, descriptors = orb.detectAndCompute(gray, None)\n",
    "\n",
    "    # Skip movement detection for the first 10 seconds\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time < skip_time:\n",
    "        cv2.putText(frame, \"Initializing...\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "    else:\n",
    "        if descriptors is not None and prev_descriptors is not None:\n",
    "            # Match descriptors\n",
    "            matches = bf.match(prev_descriptors, descriptors)\n",
    "            matches = sorted(matches, key=lambda x: x.distance)  # Sort by best match\n",
    "\n",
    "            # Calculate average displacement of keypoints\n",
    "            motion_x = []\n",
    "            motion_y = []\n",
    "            \n",
    "            for m in matches:\n",
    "                pt1 = prev_keypoints[m.queryIdx].pt  # Previous keypoint\n",
    "                pt2 = keypoints[m.trainIdx].pt  # Current keypoint\n",
    "\n",
    "                motion_x.append(pt2[0] - pt1[0])  # X movement\n",
    "                motion_y.append(pt2[1] - pt1[1])  # Y movement\n",
    "\n",
    "            if motion_x and motion_y:\n",
    "                avg_x = np.mean(motion_x)  # Average movement in X\n",
    "                avg_y = np.mean(motion_y)  # Average movement in Y\n",
    "                total_movement = np.sqrt(avg_x**2 + avg_y**2)\n",
    "\n",
    "                # Set threshold for significant movement\n",
    "                if total_movement > 120:  # Keep the original threshold\n",
    "                    movement_text = \"Camera Moving\"\n",
    "                    color = (0, 0, 255)  # Red\n",
    "                else:\n",
    "                    movement_text = \"Camera Static\"\n",
    "                    color = (0, 255, 0)  # Green\n",
    "\n",
    "                    # Plot red dot in a straight line when the camera is static\n",
    "                    if dot_position < canvas_width - 50:  # Ensure dots don't overflow\n",
    "                        cv2.circle(canvas, (dot_position, canvas_height // 2), 5, (0, 0, 255), -1)\n",
    "                        dot_position += dot_spacing  # Move position for next dot (closer spacing)\n",
    "                    else:\n",
    "                        dot_position = 50  # Reset dots to start when reaching the end\n",
    "\n",
    "                # Display movement status\n",
    "                cv2.putText(frame, movement_text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the main video feed\n",
    "    cv2.imshow(\"Camera Movement Detection\", frame)\n",
    "\n",
    "    # Show the dot plotting canvas\n",
    "    cv2.imshow(\"Static Camera Plot\", canvas)\n",
    "\n",
    "    # Update previous frame data\n",
    "    prev_gray = gray.copy()\n",
    "    prev_keypoints, prev_descriptors = keypoints, descriptors\n",
    "\n",
    "    # Press 'q' to exit the video\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Replace with your phone's IP address and port\n",
    "url = \"http://192.168.185.1:8080/video\"  # Ensure HTTP, not HTTPS\n",
    "\n",
    "# Function to handle reconnection\n",
    "def reconnect_camera():\n",
    "    while True:\n",
    "        cap = cv2.VideoCapture(url)\n",
    "        if cap.isOpened():\n",
    "            print(\"Reconnected to the camera.\")\n",
    "            return cap\n",
    "        print(\"Reconnecting to the camera...\")\n",
    "        cv2.waitKey(1000)  # Wait 1 sec before retrying\n",
    "\n",
    "# Initialize video capture\n",
    "cap = reconnect_camera()\n",
    "\n",
    "# Initialize ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Read the first frame and detect keypoints\n",
    "ret, prev_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error: Could not read the first frame.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "prev_keypoints, prev_descriptors = orb.detectAndCompute(prev_gray, None)\n",
    "\n",
    "# BFMatcher for feature matching\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Canvas for plotting dots\n",
    "canvas_height, canvas_width = 300, 800  # Canvas size\n",
    "canvas = np.ones((canvas_height, canvas_width, 3), dtype=np.uint8) * 255  # White background\n",
    "\n",
    "dot_position_x, dot_position_y = 50, canvas_height // 2  # Initial dot position\n",
    "dot_spacing = 10  # Dots appear only as frames progress\n",
    "direction = 0  # 0 = Right, 1 = Down, 2 = Left, 3 = Up\n",
    "frames_since_last_dot = 0  # Ensures dots appear as frames progress\n",
    "\n",
    "# Timer to skip first 10 seconds\n",
    "start_time = time.time()\n",
    "skip_time = 10  # Seconds to skip movement detection\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Camera disconnected. Attempting to reconnect...\")\n",
    "        cap = reconnect_camera()\n",
    "        continue  # Skip frame processing until reconnection\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect keypoints and descriptors in the new frame\n",
    "    keypoints, descriptors = orb.detectAndCompute(gray, None)\n",
    "\n",
    "    # Skip movement detection for the first 10 seconds\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time < skip_time:\n",
    "        cv2.putText(frame, \"Initializing...\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "    else:\n",
    "        if descriptors is not None and prev_descriptors is not None:\n",
    "            # Match descriptors\n",
    "            matches = bf.match(prev_descriptors, descriptors)\n",
    "            matches = sorted(matches, key=lambda x: x.distance)  # Sort by best match\n",
    "\n",
    "            # Calculate average displacement of keypoints\n",
    "            motion_x = []\n",
    "            motion_y = []\n",
    "            \n",
    "            for m in matches:\n",
    "                pt1 = prev_keypoints[m.queryIdx].pt  # Previous keypoint\n",
    "                pt2 = keypoints[m.trainIdx].pt  # Current keypoint\n",
    "\n",
    "                motion_x.append(pt2[0] - pt1[0])  # X movement\n",
    "                motion_y.append(pt2[1] - pt1[1])  # Y movement\n",
    "\n",
    "            if motion_x and motion_y:\n",
    "                avg_x = np.mean(motion_x)  # Average movement in X\n",
    "                avg_y = np.mean(motion_y)  # Average movement in Y\n",
    "                total_movement = np.sqrt(avg_x**2 + avg_y**2)\n",
    "\n",
    "                # Set threshold for significant movement\n",
    "                if total_movement > 120:  \n",
    "                    movement_text = \"Camera Moving\"\n",
    "                    color = (0, 0, 255)  # Red\n",
    "\n",
    "                    # Rotate direction on movement but keep dot at last position\n",
    "                    direction = (direction + 1) % 4  \n",
    "\n",
    "                else:\n",
    "                    movement_text = \"Camera Static\"\n",
    "                    color = (0, 255, 0)  # Green\n",
    "\n",
    "                    # Add a dot only when enough frames have passed (not continuous)\n",
    "                    frames_since_last_dot += 1\n",
    "                    if frames_since_last_dot >= 5:  # Adjust for spacing\n",
    "                        cv2.circle(canvas, (dot_position_x, dot_position_y), 5, (0, 0, 255), -1)\n",
    "\n",
    "                        # Move position based on direction\n",
    "                        if direction == 0:  # Right\n",
    "                            dot_position_x += dot_spacing\n",
    "                            if dot_position_x >= canvas_width - 50:\n",
    "                                dot_position_x = canvas_width - 50\n",
    "                        elif direction == 1:  # Down\n",
    "                            dot_position_y += dot_spacing\n",
    "                            if dot_position_y >= canvas_height - 50:\n",
    "                                dot_position_y = canvas_height - 50\n",
    "                        elif direction == 2:  # Left\n",
    "                            dot_position_x -= dot_spacing\n",
    "                            if dot_position_x <= 50:\n",
    "                                dot_position_x = 50\n",
    "                        elif direction == 3:  # Up\n",
    "                            dot_position_y -= dot_spacing\n",
    "                            if dot_position_y <= 50:\n",
    "                                dot_position_y = 50\n",
    "\n",
    "                        frames_since_last_dot = 0  # Reset counter after drawing a dot\n",
    "\n",
    "                # Display movement status\n",
    "                cv2.putText(frame, movement_text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the main video feed\n",
    "    cv2.imshow(\"Camera Movement Detection\", frame)\n",
    "\n",
    "    # Show the dot plotting canvas\n",
    "    cv2.imshow(\"Static Camera Plot\", canvas)\n",
    "\n",
    "    # Update previous frame data\n",
    "    prev_gray = gray.copy()\n",
    "    prev_keypoints, prev_descriptors = keypoints, descriptors\n",
    "\n",
    "    # Press 'q' to exit the video\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Replace with your phone's IP address and port\n",
    "url = \"http://192.168.185.1:8080/video\"  # Ensure HTTP, not HTTPS\n",
    "\n",
    "# Function to handle reconnection\n",
    "def reconnect_camera():\n",
    "    while True:\n",
    "        cap = cv2.VideoCapture(url)\n",
    "        if cap.isOpened():\n",
    "            print(\"Reconnected to the camera.\")\n",
    "            return cap\n",
    "        print(\"Reconnecting to the camera...\")\n",
    "        cv2.waitKey(1000)  # Wait 1 sec before retrying\n",
    "\n",
    "# Initialize video capture\n",
    "cap = reconnect_camera()\n",
    "\n",
    "# Initialize ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Read the first frame and detect keypoints\n",
    "ret, prev_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error: Could not read the first frame.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "prev_keypoints, prev_descriptors = orb.detectAndCompute(prev_gray, None)\n",
    "\n",
    "# BFMatcher for feature matching\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Canvas for plotting dots\n",
    "canvas_height, canvas_width = 300, 800  # Canvas size\n",
    "canvas = np.ones((canvas_height, canvas_width, 3), dtype=np.uint8) * 255  # White background\n",
    "\n",
    "dot_position_x, dot_position_y = 50, canvas_height // 2  # Initial dot position\n",
    "dot_spacing = 10  # Dots appear only as frames progress\n",
    "direction = 0  # 0 = Right, 1 = Down, 2 = Left, 3 = Up\n",
    "frames_since_last_dot = 0  # Ensures dots appear as frames progress\n",
    "\n",
    "# Timer to skip first 10 seconds\n",
    "start_time = time.time()\n",
    "skip_time = 10  # Seconds to skip movement detection\n",
    "last_rotation_time = 0  # Timer for 5s delay between rotations\n",
    "rotation_delay = 5  # 5 seconds delay before next rotation\n",
    "\n",
    "# Store drawn points to prevent intersections\n",
    "drawn_points = set()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Camera disconnected. Attempting to reconnect...\")\n",
    "        cap = reconnect_camera()\n",
    "        continue  # Skip frame processing until reconnection\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect keypoints and descriptors in the new frame\n",
    "    keypoints, descriptors = orb.detectAndCompute(gray, None)\n",
    "\n",
    "    # Skip movement detection for the first 10 seconds\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time < skip_time:\n",
    "        cv2.putText(frame, \"Initializing...\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "    else:\n",
    "        if descriptors is not None and prev_descriptors is not None:\n",
    "            # Match descriptors\n",
    "            matches = bf.match(prev_descriptors, descriptors)\n",
    "            matches = sorted(matches, key=lambda x: x.distance)  # Sort by best match\n",
    "\n",
    "            # Calculate average displacement of keypoints\n",
    "            motion_x = []\n",
    "            motion_y = []\n",
    "            \n",
    "            for m in matches:\n",
    "                pt1 = prev_keypoints[m.queryIdx].pt  # Previous keypoint\n",
    "                pt2 = keypoints[m.trainIdx].pt  # Current keypoint\n",
    "\n",
    "                motion_x.append(pt2[0] - pt1[0])  # X movement\n",
    "                motion_y.append(pt2[1] - pt1[1])  # Y movement\n",
    "\n",
    "            if motion_x and motion_y:\n",
    "                avg_x = np.mean(motion_x)  # Average movement in X\n",
    "                avg_y = np.mean(motion_y)  # Average movement in Y\n",
    "                total_movement = np.sqrt(avg_x**2 + avg_y**2)\n",
    "\n",
    "                # Set threshold for significant movement\n",
    "                if total_movement > 120:  \n",
    "                    movement_text = \"Camera Moving\"\n",
    "                    color = (0, 0, 255)  # Red\n",
    "\n",
    "                    # Rotate direction only if delay period has passed\n",
    "                    current_time = time.time()\n",
    "                    if current_time - last_rotation_time >= rotation_delay:\n",
    "                        direction = (direction + 1) % 4  # Rotate direction\n",
    "                        last_rotation_time = current_time  # Update last rotation time\n",
    "\n",
    "                else:\n",
    "                    movement_text = \"Camera Static\"\n",
    "                    color = (0, 255, 0)  # Green\n",
    "\n",
    "                    # Add a dot only when enough frames have passed (not continuous)\n",
    "                    frames_since_last_dot += 1\n",
    "                    if frames_since_last_dot >= 5:  # Adjust for spacing\n",
    "                        # Prevent intersections by checking if the position is already occupied\n",
    "                        if (dot_position_x, dot_position_y) not in drawn_points:\n",
    "                            cv2.circle(canvas, (dot_position_x, dot_position_y), 5, (0, 0, 255), -1)\n",
    "                            drawn_points.add((dot_position_x, dot_position_y))  # Store the point\n",
    "\n",
    "                            # Move position based on direction\n",
    "                            if direction == 0:  # Right\n",
    "                                dot_position_x += dot_spacing\n",
    "                                if dot_position_x >= canvas_width - 50:\n",
    "                                    dot_position_x = canvas_width - 50\n",
    "                            elif direction == 1:  # Down\n",
    "                                dot_position_y += dot_spacing\n",
    "                                if dot_position_y >= canvas_height - 50:\n",
    "                                    dot_position_y = canvas_height - 50\n",
    "                            elif direction == 2:  # Left\n",
    "                                dot_position_x -= dot_spacing\n",
    "                                if dot_position_x <= 50:\n",
    "                                    dot_position_x = 50\n",
    "                            elif direction == 3:  # Up\n",
    "                                dot_position_y -= dot_spacing\n",
    "                                if dot_position_y <= 50:\n",
    "                                    dot_position_y = 50\n",
    "\n",
    "                        frames_since_last_dot = 0  # Reset counter after drawing a dot\n",
    "\n",
    "                # Display movement status\n",
    "                cv2.putText(frame, movement_text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the main video feed\n",
    "    cv2.imshow(\"Camera Movement Detection\", frame)\n",
    "\n",
    "    # Show the dot plotting canvas\n",
    "    cv2.imshow(\"Static Camera Plot\", canvas)\n",
    "\n",
    "    # Update previous frame data\n",
    "    prev_gray = gray.copy()\n",
    "    prev_keypoints, prev_descriptors = keypoints, descriptors\n",
    "\n",
    "    # Press 'q' to exit the video\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconnected to the camera.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 20:43:06.016 python[13299:579998] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-08 20:43:06.016 python[13299:579998] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "[mjpeg @ 0x116772c50] overread 8\n",
      "[mjpeg @ 0x116772c50] overread 8\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Replace with your phone's IP address and port\n",
    "url = \"http://192.168.185.1:8080/video\"  # Ensure HTTP, not HTTPS\n",
    "\n",
    "# Function to handle reconnection\n",
    "def reconnect_camera():\n",
    "    while True:\n",
    "        cap = cv2.VideoCapture(url)\n",
    "        if cap.isOpened():\n",
    "            print(\"Reconnected to the camera.\")\n",
    "            return cap\n",
    "        print(\"Reconnecting to the camera...\")\n",
    "        cv2.waitKey(1000)  # Wait 1 sec before retrying\n",
    "\n",
    "# Initialize video capture\n",
    "cap = reconnect_camera()\n",
    "\n",
    "# Initialize ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Read the first frame and detect keypoints\n",
    "ret, prev_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error: Could not read the first frame.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "prev_keypoints, prev_descriptors = orb.detectAndCompute(prev_gray, None)\n",
    "\n",
    "# BFMatcher for feature matching\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Get full-screen resolution\n",
    "screen_width = 1920\n",
    "screen_height = 1080\n",
    "\n",
    "# Canvas for plotting dots (Full-screen size)\n",
    "canvas = np.ones((screen_height, screen_width, 3), dtype=np.uint8) * 255  # White background\n",
    "\n",
    "dot_position_x, dot_position_y = 50, 50  # Start at the **top-left corner**\n",
    "dot_spacing = 20  # Increased spacing to slow down plotting\n",
    "direction = 0  # 0 = Right, 1 = Down, 2 = Left, 3 = Up\n",
    "frames_since_last_dot = 0  # Ensures dots appear as frames progress\n",
    "\n",
    "# Timer to skip first 10 seconds\n",
    "start_time = time.time()\n",
    "skip_time = 10  # Seconds to skip movement detection\n",
    "last_rotation_time = 0  # Timer for 5s delay between rotations\n",
    "rotation_delay = 8  # 5 seconds delay before next rotation\n",
    "\n",
    "# Store drawn points to prevent intersections\n",
    "drawn_points = set()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Camera disconnected. Attempting to reconnect...\")\n",
    "        cap = reconnect_camera()\n",
    "        continue  # Skip frame processing until reconnection\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect keypoints and descriptors in the new frame\n",
    "    keypoints, descriptors = orb.detectAndCompute(gray, None)\n",
    "\n",
    "    # Skip movement detection for the first 10 seconds\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time < skip_time:\n",
    "        cv2.putText(frame, \"Initializing...\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "    else:\n",
    "        if descriptors is not None and prev_descriptors is not None:\n",
    "            # Match descriptors\n",
    "            matches = bf.match(prev_descriptors, descriptors)\n",
    "            matches = sorted(matches, key=lambda x: x.distance)  # Sort by best match\n",
    "\n",
    "            # Calculate average displacement of keypoints\n",
    "            motion_x = []\n",
    "            motion_y = []\n",
    "            \n",
    "            for m in matches:\n",
    "                pt1 = prev_keypoints[m.queryIdx].pt  # Previous keypoint\n",
    "                pt2 = keypoints[m.trainIdx].pt  # Current keypoint\n",
    "\n",
    "                motion_x.append(pt2[0] - pt1[0])  # X movement\n",
    "                motion_y.append(pt2[1] - pt1[1])  # Y movement\n",
    "\n",
    "            if motion_x and motion_y:\n",
    "                avg_x = np.mean(motion_x)  # Average movement in X\n",
    "                avg_y = np.mean(motion_y)  # Average movement in Y\n",
    "                total_movement = np.sqrt(avg_x**2 + avg_y**2)\n",
    "\n",
    "                # Set threshold for significant movement\n",
    "                if total_movement > 200:  \n",
    "                    movement_text = \"Camera Moving\"\n",
    "                    color = (0, 0, 255)  # Red\n",
    "\n",
    "                    # Rotate direction only if delay period has passed\n",
    "                    current_time = time.time()\n",
    "                    if current_time - last_rotation_time >= rotation_delay:\n",
    "                        direction = (direction + 1) % 4  # Rotate direction\n",
    "                        last_rotation_time = current_time  # Update last rotation time\n",
    "\n",
    "                else:\n",
    "                    movement_text = \"Camera Static\"\n",
    "                    color = (0, 255, 0)  # Green\n",
    "\n",
    "                    # Add a dot only when enough frames have passed (not continuous)\n",
    "                    frames_since_last_dot += 1\n",
    "                    if frames_since_last_dot >= 6:  # Slower plotting speed\n",
    "                        # Prevent intersections by checking if the position is already occupied\n",
    "                        if (dot_position_x, dot_position_y) not in drawn_points:\n",
    "                            cv2.circle(canvas, (dot_position_x, dot_position_y), 5, (0, 0, 255), -1)\n",
    "                            drawn_points.add((dot_position_x, dot_position_y))  # Store the point\n",
    "\n",
    "                            # Move position based on direction\n",
    "                            if direction == 0:  # Right\n",
    "                                dot_position_x += dot_spacing\n",
    "                                if dot_position_x >= screen_width - 50:\n",
    "                                    direction = (direction + 1) % 4  # Turn down\n",
    "                            elif direction == 1:  # Down\n",
    "                                dot_position_y += dot_spacing\n",
    "                                if dot_position_y >= screen_height - 50:\n",
    "                                    direction = (direction + 1) % 4  # Turn left\n",
    "                            elif direction == 2:  # Left\n",
    "                                dot_position_x -= dot_spacing\n",
    "                                if dot_position_x <= 50:\n",
    "                                    direction = (direction + 1) % 4  # Turn up\n",
    "                            elif direction == 3:  # Up\n",
    "                                dot_position_y -= dot_spacing\n",
    "                                if dot_position_y <= 50:\n",
    "                                    direction = (direction + 1) % 4  # Turn right\n",
    "\n",
    "                        frames_since_last_dot = 0  # Reset counter after drawing a dot\n",
    "\n",
    "                # Display movement status\n",
    "                cv2.putText(frame, movement_text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the main video feed\n",
    "    cv2.imshow(\"Camera Movement Detection\", frame)\n",
    "\n",
    "    # Show the dot plotting canvas in full screen\n",
    "    cv2.namedWindow(\"Static Camera Plot\", cv2.WND_PROP_FULLSCREEN)\n",
    "    cv2.setWindowProperty(\"Static Camera Plot\", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "    cv2.imshow(\"Static Camera Plot\", canvas)\n",
    "\n",
    "    # Update previous frame data\n",
    "    prev_gray = gray.copy()\n",
    "    prev_keypoints, prev_descriptors = keypoints, descriptors\n",
    "\n",
    "    # Press 'q' to exit the video\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconnected to the camera.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x3047ffc90] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final frame saved as saved_frames/final_frame.jpeg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Replace with your phone's IP address and port\n",
    "url = \"http://192.168.185.1:8080/video\"  # Ensure HTTP, not HTTPS\n",
    "\n",
    "# Function to handle reconnection\n",
    "def reconnect_camera():\n",
    "    while True:\n",
    "        cap = cv2.VideoCapture(url)\n",
    "        if cap.isOpened():\n",
    "            print(\"Reconnected to the camera.\")\n",
    "            return cap\n",
    "        print(\"Reconnecting to the camera...\")\n",
    "        cv2.waitKey(1000)  # Wait 1 sec before retrying\n",
    "\n",
    "# Initialize video capture\n",
    "cap = reconnect_camera()\n",
    "\n",
    "# Initialize ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Read the first frame and detect keypoints\n",
    "ret, prev_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error: Could not read the first frame.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "prev_keypoints, prev_descriptors = orb.detectAndCompute(prev_gray, None)\n",
    "\n",
    "# BFMatcher for feature matching\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Get screen resolution for full-screen canvas\n",
    "screen_width = 1920  # Adjust based on display resolution\n",
    "screen_height = 1080\n",
    "canvas = np.ones((screen_height, screen_width, 3), dtype=np.uint8) * 255  # Full-screen white canvas\n",
    "\n",
    "dot_position_x, dot_position_y = 50, 50  # Start at the **top-left corner**\n",
    "dot_spacing = 20  # Slower speed by increasing spacing\n",
    "direction = 0  # 0 = Right, 1 = Down, 2 = Left, 3 = Up\n",
    "frames_since_last_dot = 0  # Slows down dot plotting\n",
    "\n",
    "# Timer to skip first 10 seconds\n",
    "start_time = time.time()\n",
    "skip_time = 10  # Seconds to skip movement detection\n",
    "last_rotation_time = 0  # Timer for 5s delay between rotations\n",
    "rotation_delay = 8  # 5 seconds delay before next rotation\n",
    "\n",
    "# Store drawn points to prevent intersections\n",
    "drawn_points = set()\n",
    "\n",
    "# Directory to save frames\n",
    "frame_save_path = \"saved_frames\"\n",
    "os.makedirs(frame_save_path, exist_ok=True)  # Create directory for saved frames\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Camera disconnected. Attempting to reconnect...\")\n",
    "        cap = reconnect_camera()\n",
    "        continue  # Skip frame processing until reconnection\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect keypoints and descriptors in the new frame\n",
    "    keypoints, descriptors = orb.detectAndCompute(gray, None)\n",
    "\n",
    "    # Skip movement detection for the first 10 seconds\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time < skip_time:\n",
    "        cv2.putText(frame, \"Initializing...\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "    else:\n",
    "        if descriptors is not None and prev_descriptors is not None:\n",
    "            # Match descriptors\n",
    "            matches = bf.match(prev_descriptors, descriptors)\n",
    "            matches = sorted(matches, key=lambda x: x.distance)  # Sort by best match\n",
    "\n",
    "            # Calculate average displacement of keypoints\n",
    "            motion_x = []\n",
    "            motion_y = []\n",
    "            \n",
    "            for m in matches:\n",
    "                pt1 = prev_keypoints[m.queryIdx].pt  # Previous keypoint\n",
    "                pt2 = keypoints[m.trainIdx].pt  # Current keypoint\n",
    "\n",
    "                motion_x.append(pt2[0] - pt1[0])  # X movement\n",
    "                motion_y.append(pt2[1] - pt1[1])  # Y movement\n",
    "\n",
    "            if motion_x and motion_y:\n",
    "                avg_x = np.mean(motion_x)  # Average movement in X\n",
    "                avg_y = np.mean(motion_y)  # Average movement in Y\n",
    "                total_movement = np.sqrt(avg_x**2 + avg_y**2)\n",
    "\n",
    "                # Set threshold for significant movement\n",
    "                if total_movement > 200:  \n",
    "                    movement_text = \"Camera Moving\"\n",
    "                    color = (0, 0, 255)  # Red\n",
    "\n",
    "                    # Rotate direction only if delay period has passed\n",
    "                    current_time = time.time()\n",
    "                    if current_time - last_rotation_time >= rotation_delay:\n",
    "                        direction = (direction + 1) % 4  # Rotate direction\n",
    "                        last_rotation_time = current_time  # Update last rotation time\n",
    "\n",
    "                else:\n",
    "                    movement_text = \"Camera Static\"\n",
    "                    color = (0, 255, 0)  # Green\n",
    "\n",
    "                    # Add a dot only when enough frames have passed (slower plotting)\n",
    "                    frames_since_last_dot += 1\n",
    "                    if frames_since_last_dot >= 6:  # Slower by increasing frames delay\n",
    "                        # Prevent intersections by checking if the position is already occupied\n",
    "                        if (dot_position_x, dot_position_y) not in drawn_points:\n",
    "                            cv2.circle(canvas, (dot_position_x, dot_position_y), 5, (0, 0, 255), -1)\n",
    "                            drawn_points.add((dot_position_x, dot_position_y))  # Store the point\n",
    "\n",
    "                            # Save frame with filename as coordinates\n",
    "                            frame_name = f\"{dot_position_x}_{dot_position_y}.jpeg\"\n",
    "                            frame_path = os.path.join(frame_save_path, frame_name)\n",
    "                            cv2.imwrite(frame_path, frame)  # Save the frame\n",
    "\n",
    "                            # Move position based on direction\n",
    "                            if direction == 0:  # Right\n",
    "                                dot_position_x += dot_spacing\n",
    "                                if dot_position_x >= screen_width - 50:\n",
    "                                    direction = (direction + 1) % 4  # Turn down\n",
    "                            elif direction == 1:  # Down\n",
    "                                dot_position_y += dot_spacing\n",
    "                                if dot_position_y >= screen_height - 50:\n",
    "                                    direction = (direction + 1) % 4  # Turn left\n",
    "                            elif direction == 2:  # Left\n",
    "                                dot_position_x -= dot_spacing\n",
    "                                if dot_position_x <= 50:\n",
    "                                    direction = (direction + 1) % 4  # Turn up\n",
    "                            elif direction == 3:  # Up\n",
    "                                dot_position_y -= dot_spacing\n",
    "                                if dot_position_y <= 50:\n",
    "                                    direction = (direction + 1) % 4  # Turn right\n",
    "\n",
    "                        frames_since_last_dot = 0  # Reset counter after drawing a dot\n",
    "\n",
    "                # Display movement status\n",
    "                cv2.putText(frame, movement_text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the main video feed\n",
    "    cv2.imshow(\"Camera Movement Detection\", frame)\n",
    "\n",
    "    # Show the dot plotting canvas in fullscreen mode\n",
    "    cv2.namedWindow(\"Static Camera Plot\", cv2.WND_PROP_FULLSCREEN)\n",
    "    cv2.setWindowProperty(\"Static Camera Plot\", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "    cv2.imshow(\"Static Camera Plot\", canvas)\n",
    "\n",
    "    # Update previous frame data\n",
    "    prev_gray = gray.copy()\n",
    "    prev_keypoints, prev_descriptors = keypoints, descriptors\n",
    "\n",
    "    # Press 'q' to exit the video\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Save final canvas as an image\n",
    "final_frame_path = os.path.join(frame_save_path, \"final_frame.jpeg\")\n",
    "cv2.imwrite(final_frame_path, canvas)\n",
    "print(f\"Final frame saved as {final_frame_path}\")\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: opencv-python in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (2.0.2)\n",
      "Requirement already satisfied: torchvision in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: pillow in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (11.1.0)\n",
      "Requirement already satisfied: filelock in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# crack detection YOLO\n",
    "#%pip install torch opencv-python numpy torchvision pillow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (8.3.73)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from ultralytics) (2.0.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from ultralytics) (3.10.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from ultralytics) (1.15.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from ultralytics) (2.6.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from ultralytics) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from ultralytics) (6.1.1)\n",
      "Requirement already satisfied: py-cpuinfo in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2024.12.14)\n",
      "Requirement already satisfied: filelock in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pulkit/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%.\\crack_detection\\Scripts\\activate\n",
    "#%pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = \"best.pt\" \n",
    "#model = YOLO(model_path).to(\"cpu\")\n",
    "\n",
    "#def yolov8(image_path):\n",
    "    #image_path = \"saved_frames/370_250.jpeg\"  # Replace with your image path\n",
    "    #frame = cv2.imread(image_path)\n",
    "    \n",
    "    # Run YOLOv8 inference on the frame\n",
    "    #results = model.predict(frame)  # You can also pass a path to a frame image if you need\n",
    "    #print(len(results[0].boxes.cls))\n",
    "    #print(results[0].names)\n",
    "    # Process results (boxes, labels, and confidence scores)\n",
    "    #boxes = results[0].boxes  # Bounding box data for detected objects\n",
    "    #for box in boxes:\n",
    "    #    # Draw bounding boxes around detected cracks\n",
    "    #    x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get the coordinates (x1, y1, x2, y2) of the box\n",
    "    #    conf = box.conf[0]  # Get the confidence of the detection\n",
    "        \n",
    "        # Draw a rectangle and label (optional)\n",
    "    #    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green box\n",
    "    #    cv2.putText(frame, f'{conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the resulting frame with detected cracks\n",
    "    # cv2.imshow(\"Crack Detection\", frame)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "    \n",
    "#yolov8(\"saved_frames/70_390.jpeg\")\n",
    "#yolov8(\"saved_frames/50_610.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 cracks, 67.5ms\n",
      "Speed: 2.1ms preprocess, 67.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cracks, 59.9ms\n",
      "Speed: 1.9ms preprocess, 59.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 59.8ms\n",
      "Speed: 1.7ms preprocess, 59.8ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 61.2ms\n",
      "Speed: 1.7ms preprocess, 61.2ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 61.9ms\n",
      "Speed: 1.7ms preprocess, 61.9ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 60.7ms\n",
      "Speed: 1.6ms preprocess, 60.7ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cracks, 63.1ms\n",
      "Speed: 1.6ms preprocess, 63.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cracks, 62.8ms\n",
      "Speed: 1.5ms preprocess, 62.8ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 62.2ms\n",
      "Speed: 1.7ms preprocess, 62.2ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 64.2ms\n",
      "Speed: 1.8ms preprocess, 64.2ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 65.5ms\n",
      "Speed: 1.7ms preprocess, 65.5ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 57.9ms\n",
      "Speed: 1.6ms preprocess, 57.9ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cracks, 61.4ms\n",
      "Speed: 1.8ms preprocess, 61.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 57.8ms\n",
      "Speed: 1.4ms preprocess, 57.8ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 56.0ms\n",
      "Speed: 1.5ms preprocess, 56.0ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cracks, 60.1ms\n",
      "Speed: 1.8ms preprocess, 60.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 crack, 60.0ms\n",
      "Speed: 1.5ms preprocess, 60.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cracks, 57.7ms\n",
      "Speed: 1.5ms preprocess, 57.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 61.6ms\n",
      "Speed: 1.4ms preprocess, 61.6ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 57.0ms\n",
      "Speed: 1.4ms preprocess, 57.0ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 59.1ms\n",
      "Speed: 1.6ms preprocess, 59.1ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 59.2ms\n",
      "Speed: 1.6ms preprocess, 59.2ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cracks, 61.1ms\n",
      "Speed: 1.7ms preprocess, 61.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 crack, 62.7ms\n",
      "Speed: 2.0ms preprocess, 62.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cracks, 59.7ms\n",
      "Speed: 1.9ms preprocess, 59.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cracks, 57.8ms\n",
      "Speed: 1.5ms preprocess, 57.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cracks, 60.4ms\n",
      "Speed: 1.8ms preprocess, 60.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 58.7ms\n",
      "Speed: 1.9ms preprocess, 58.7ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cracks, 58.1ms\n",
      "Speed: 1.5ms preprocess, 58.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cracks, 57.1ms\n",
      "Speed: 1.6ms preprocess, 57.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 56.9ms\n",
      "Speed: 1.8ms preprocess, 56.9ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 58.5ms\n",
      "Speed: 1.8ms preprocess, 58.5ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 58.8ms\n",
      "Speed: 1.6ms preprocess, 58.8ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 58.0ms\n",
      "Speed: 1.6ms preprocess, 58.0ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cracks, 55.7ms\n",
      "Speed: 1.4ms preprocess, 55.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 58.1ms\n",
      "Speed: 1.7ms preprocess, 58.1ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 58.0ms\n",
      "Speed: 1.6ms preprocess, 58.0ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Directory containing saved frames\n",
    "frame_save_path = \"saved_frames\"\n",
    "final_frame_path = os.path.join(frame_save_path, \"final_frame.jpeg\")\n",
    "\n",
    "# Check if final frame exists\n",
    "if not os.path.exists(final_frame_path):\n",
    "    print(\"Error: final_frame.jpeg not found.\")\n",
    "    exit()\n",
    "\n",
    "# Load the final frame\n",
    "final_frame = cv2.imread(final_frame_path)\n",
    "\n",
    "# Read all filenames from saved_frames directory\n",
    "dot_filenames = [f for f in os.listdir(frame_save_path) if f.endswith(\".jpeg\") and f != \"final_frame.jpeg\"]\n",
    "\n",
    "model_path = \"best.pt\" \n",
    "model = YOLO(model_path).to(\"cpu\")\n",
    "\n",
    "# Define green color\n",
    "green_color = (0, 255, 0)  # Green\n",
    "\n",
    "# Draw \"X\" at each dot's saved coordinate in green\n",
    "for filename in dot_filenames:\n",
    "    try:\n",
    "        file_path = frame_save_path+\"/\"+filename\n",
    "        frame = cv2.imread(file_path)\n",
    "        results = model.predict(frame)  # You can also pass a path to a frame image if you need\n",
    "        c = len(results[0].boxes.cls)\n",
    "        if c >0:\n",
    "\n",
    "            # Extract coordinates from filename (format: \"X_Y.jpeg\")\n",
    "            coords = filename.replace(\".jpeg\", \"\").split(\"_\")\n",
    "            x, y = int(coords[0]), int(coords[1])\n",
    "            if x<370:\n",
    "                # Draw \"X\" at the coordinates in green\n",
    "                cv2.line(final_frame, (x - 5, y - 5), (x + 5, y + 5), green_color, 2)\n",
    "                cv2.line(final_frame, (x - 5, y + 5), (x + 5, y - 5), green_color, 2)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Save and display the updated final frame\n",
    "updated_final_frame_path = os.path.join(frame_save_path, \"final_frame_with_x.jpeg\")\n",
    "cv2.imwrite(updated_final_frame_path, final_frame)\n",
    "#print(f\"Updated final frame saved as {updated_final_frame_path}\")\n",
    "\n",
    "# Show the result\n",
    "cv2.imshow(\"Final Frame with Green X Marks\", final_frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
